{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389fa67e-0d0e-49e3-ae97-3989147f2189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /Users/jorgemartinez/Desktop/7_FullStack/Final_Project/1_Datasets\n",
      "- .DS_Store\n",
      "- 6_MentalChat_16K\n",
      "- 3_EmpatheticDialogues_Facebook_AI\n",
      "- 5_Suicide_and_DepressionDetection\n",
      "- Evidence_based_Proprietary\n",
      "- 2_DailyDialog\n",
      "- 4_mental_health_counseling_conversations\n",
      "- 1_goemotions_dataset\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# I am importing the main libraries I will use for cleaning and merging\n",
    "# - pathlib for working with file paths\n",
    "# - pandas and numpy for data handling\n",
    "\n",
    "# I am setting the base directory where all my datasets live\n",
    "BASE_DIR = Path.home() / \"Desktop\" / \"7_FullStack\" / \"Final_Project\" / \"1_Datasets\"\n",
    "\n",
    "# I am checking that Python can see the dataset folders\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "for p in BASE_DIR.iterdir():\n",
    "    print(\"-\", p.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e517c4c-ce5a-4a37-999f-fa0023db7ecc",
   "metadata": {},
   "source": [
    "## Define dataset folder paths and quickly inspect files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ca0449-1689-48d0-9820-3e07efeadee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoEmotions files:\n",
      "  - train.tsv\n",
      "  - dev.tsv\n",
      "  - README.md\n",
      "  - test.tsv\n",
      "  - emotions.txt\n",
      "\n",
      "DailyDialog files:\n",
      "  - validation.csv\n",
      "  - test.csv\n",
      "  - train.csv\n",
      "\n",
      "Empathetic Dialogues files:\n",
      "  - emotion_emotion_69k.csv\n",
      "\n",
      "Mental Health Counseling files:\n",
      "  - combined_dataset.json\n",
      "\n",
      "Suicide and Depression Detection files:\n",
      "  - Suicide_Detection.csv\n",
      "\n",
      "MentalChat 16K files:\n",
      "  - Synthetic_Data_10K.csv\n",
      "  - README.md\n",
      "  - Interview_Data_6K.csv\n"
     ]
    }
   ],
   "source": [
    "# I am defining paths for each of the main external datasets\n",
    "\n",
    "GOEMO_DIR = BASE_DIR / \"1_goemotions_dataset\"\n",
    "DAILYDIALOG_DIR = BASE_DIR / \"2_DailyDialog\"\n",
    "EMP_DIALOG_DIR = BASE_DIR / \"3_EmpatheticDialogues_Facebook_AI\"\n",
    "MH_COUNSEL_DIR = BASE_DIR / \"4_mental_health_counseling_conversations\"\n",
    "SUICIDE_DIR = BASE_DIR / \"5_Suicide_and_DepressionDetection\"\n",
    "MENTALCHAT_DIR = BASE_DIR / \"6_MentalChat_16K\"\n",
    "\n",
    "# I am checking the files inside each dataset folder\n",
    "\n",
    "print(\"GoEmotions files:\")\n",
    "for p in GOEMO_DIR.iterdir():\n",
    "    print(\"  -\", p.name)\n",
    "\n",
    "print(\"\\nDailyDialog files:\")\n",
    "for p in DAILYDIALOG_DIR.iterdir():\n",
    "    print(\"  -\", p.name)\n",
    "\n",
    "print(\"\\nEmpathetic Dialogues files:\")\n",
    "for p in EMP_DIALOG_DIR.iterdir():\n",
    "    print(\"  -\", p.name)\n",
    "\n",
    "print(\"\\nMental Health Counseling files:\")\n",
    "for p in MH_COUNSEL_DIR.iterdir():\n",
    "    print(\"  -\", p.name)\n",
    "\n",
    "print(\"\\nSuicide and Depression Detection files:\")\n",
    "for p in SUICIDE_DIR.iterdir():\n",
    "    print(\"  -\", p.name)\n",
    "\n",
    "print(\"\\nMentalChat 16K files:\")\n",
    "for p in MENTALCHAT_DIR.iterdir():\n",
    "    print(\"  -\", p.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f8fc4-ad44-4ff6-8065-9c87c7b17310",
   "metadata": {},
   "source": [
    "# Now all external dataset folders are correctly detected and accessible.\n",
    "\n",
    "This means I am ready to start building the 3 master buckets we discussed:\n",
    "\n",
    "1. classification_bucket\n",
    "2. response_bucket\n",
    "3. safety_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7324f9-e5b3-4a91-a50d-6550203ec793",
   "metadata": {},
   "source": [
    "# Create the 3 master bucket DataFrames (empty)\n",
    "\n",
    "What this does:\n",
    "\n",
    "1. Defines the exact schema I need across the entire assistant.\n",
    "2. Ensures every dataset I bring in will fit into one of the buckets.\n",
    "3. Matching columns now guarantees merging them later will be painless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1505da-2d6e-4fbd-b039-3cc8de1fc34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Empty DataFrame\n",
       " Columns: [user_message, atlas_emotion, need, strategy, safety_flag, source]\n",
       " Index: [],\n",
       " Empty DataFrame\n",
       " Columns: [user_message, bot_reply, atlas_emotion, need, strategy, safety_flag, source]\n",
       " Index: [],\n",
       " Empty DataFrame\n",
       " Columns: [user_message, safety_flag, source]\n",
       " Index: [])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am creating empty DataFrames for the three main model buckets\n",
    "\n",
    "classification_bucket = pd.DataFrame(columns=[\n",
    "    \"user_message\",\n",
    "    \"atlas_emotion\",\n",
    "    \"need\",\n",
    "    \"strategy\",\n",
    "    \"safety_flag\",\n",
    "    \"source\"\n",
    "])\n",
    "\n",
    "response_bucket = pd.DataFrame(columns=[\n",
    "    \"user_message\",\n",
    "    \"bot_reply\",\n",
    "    \"atlas_emotion\",\n",
    "    \"need\",\n",
    "    \"strategy\",\n",
    "    \"safety_flag\",\n",
    "    \"source\"\n",
    "])\n",
    "\n",
    "safety_bucket = pd.DataFrame(columns=[\n",
    "    \"user_message\",\n",
    "    \"safety_flag\",\n",
    "    \"source\"\n",
    "])\n",
    "\n",
    "classification_bucket, response_bucket, safety_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75341fe9-586a-4464-9abf-0b2beff787df",
   "metadata": {},
   "source": [
    "# Create the helper function for cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6235d25e-0197-4115-a92f-8b91f2e8a117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test. With spaces and unicode .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# I am defining a helper function to clean raw text consistently across all datasets\n",
    "def clean_text(text):\n",
    "    # I am converting non-string values (like NaN) into empty strings\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # I am removing stray line breaks and tabs\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    # I am normalizing multiple spaces into one space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # I am fixing common unicode artifacts (example: \\xa0)\n",
    "    text = text.replace(\"\\xa0\", \" \").strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# I am testing the function\n",
    "clean_text(\"This   is   a   test.\\nWith spaces \\t and unicode\\xa0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346c256-72a1-4330-bbb1-715140e7db31",
   "metadata": {},
   "source": [
    "## Build the safety flag detection function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e0786-7c51-4829-9ea3-a964da8bd908",
   "metadata": {},
   "source": [
    "Create a safety flag helper\n",
    "\n",
    "I will start from a base safety level that depends on the dataset:\n",
    "\n",
    "0 = general conversation\n",
    "\n",
    "1 = mental-health related but not clearly suicidal\n",
    "\n",
    "2 = crisis / suicidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f80d98f-300f-4644-b532-7d02794b3fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# I am defining a list of keywords that indicate possible crisis or self-harm risk\n",
    "CRISIS_KEYWORDS = [\n",
    "    \"suicide\", \"suicidal\", \"kill myself\", \"kill myself.\",\n",
    "    \"end my life\", \"end it all\", \"can't go on\", \"dont want to live\",\n",
    "    \"don't want to live\", \"self-harm\", \"self harm\", \"hurt myself\",\n",
    "    \"hurting myself\", \"cutting\", \"take my life\", \"no reason to live\"\n",
    "]\n",
    "\n",
    "# I am defining a helper function to compute a safety flag for one text\n",
    "def compute_safety_flag(text, base_flag=0):\n",
    "    \"\"\"\n",
    "    base_flag:\n",
    "        0 = general\n",
    "        1 = mental-health related\n",
    "        2 = crisis (always stays 2)\n",
    "    \"\"\"\n",
    "    # I am cleaning the text first so detection is more reliable\n",
    "    cleaned = clean_text(text).lower()\n",
    "    \n",
    "    # If the base flag is already 2, I am keeping it as crisis\n",
    "    if base_flag == 2:\n",
    "        return 2\n",
    "    \n",
    "    # I am checking if any crisis keyword appears in the text\n",
    "    for kw in CRISIS_KEYWORDS:\n",
    "        if kw in cleaned:\n",
    "            return 2\n",
    "    \n",
    "    # If no crisis keyword was found, I am returning the base flag\n",
    "    return base_flag\n",
    "\n",
    "# I am doing a quick test of the safety flag function\n",
    "print(compute_safety_flag(\"Today was hard, I feel anxious.\", base_flag=1))       # expect 1\n",
    "print(compute_safety_flag(\"I want to end my life tonight.\", base_flag=1))        # expect 2\n",
    "print(compute_safety_flag(\"This is a normal conversation.\", base_flag=0))        # expect 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc70ee8-9491-4373-a24f-05ca99155a16",
   "metadata": {},
   "source": [
    "# Normalize GoEmotions into the buckets (first real dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268441c2-e186-46a6-95f4-4953f8b71359",
   "metadata": {},
   "source": [
    "## Inspect GoEmotions columns and adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1425257-ec67-42b1-a282-2976c7662a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (43410, 3)\n",
      "First 5 rows of train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>27</td>\n",
       "      <td>eebbqej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>14</td>\n",
       "      <td>ed7ypvh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>3</td>\n",
       "      <td>ed0bdzj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   1        2\n",
       "0  My favourite food is anything I didn't have to...  27  eebbqej\n",
       "1  Now if he does off himself, everyone will thin...  27  ed00q6i\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING   2  eezlygj\n",
       "3                        To make her feel threatened  14  ed7ypvh\n",
       "4                             Dirty Southern Wankers   3  ed0bdzj"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev shape: (5426, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is this in New Orleans?? I really feel like th...</td>\n",
       "      <td>27</td>\n",
       "      <td>edgurhb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You know the answer man, you are programmed to...</td>\n",
       "      <td>4,27</td>\n",
       "      <td>ee84bjg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've never been this sad in my life!</td>\n",
       "      <td>25</td>\n",
       "      <td>edcu99z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The economy is heavily controlled and subsidiz...</td>\n",
       "      <td>4,27</td>\n",
       "      <td>edc32e2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He could have easily taken a real camera from ...</td>\n",
       "      <td>20</td>\n",
       "      <td>eepig6r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0     1        2\n",
       "0  Is this in New Orleans?? I really feel like th...    27  edgurhb\n",
       "1  You know the answer man, you are programmed to...  4,27  ee84bjg\n",
       "2               I've never been this sad in my life!    25  edcu99z\n",
       "3  The economy is heavily controlled and subsidiz...  4,27  edc32e2\n",
       "4  He could have easily taken a real camera from ...    20  eepig6r"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (5427, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m really sorry about your situation :( Altho...</td>\n",
       "      <td>25</td>\n",
       "      <td>eecwqtt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's wonderful because it's awful. At not with.</td>\n",
       "      <td>0</td>\n",
       "      <td>ed5f85d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kings fan here, good luck to you guys! Will be...</td>\n",
       "      <td>13</td>\n",
       "      <td>een27c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I didn't know that, thank you for teaching me ...</td>\n",
       "      <td>15</td>\n",
       "      <td>eelgwd1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They got bored from haunting earth for thousan...</td>\n",
       "      <td>27</td>\n",
       "      <td>eem5uti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   1        2\n",
       "0  I’m really sorry about your situation :( Altho...  25  eecwqtt\n",
       "1    It's wonderful because it's awful. At not with.   0  ed5f85d\n",
       "2  Kings fan here, good luck to you guys! Will be...  13  een27c3\n",
       "3  I didn't know that, thank you for teaching me ...  15  eelgwd1\n",
       "4  They got bored from haunting earth for thousan...  27  eem5uti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I am checking the first few rows and columns of the GoEmotions train split\n",
    "go_train = pd.read_csv(GOEMO_DIR / \"train.tsv\", sep=\"\\t\", header=None)\n",
    "go_dev = pd.read_csv(GOEMO_DIR / \"dev.tsv\", sep=\"\\t\", header=None)\n",
    "go_test = pd.read_csv(GOEMO_DIR / \"test.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "print(\"Train shape:\", go_train.shape)\n",
    "print(\"First 5 rows of train:\")\n",
    "display(go_train.head())\n",
    "\n",
    "print(\"Dev shape:\", go_dev.shape)\n",
    "display(go_dev.head())\n",
    "\n",
    "print(\"Test shape:\", go_test.shape)\n",
    "display(go_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfed67-5dbf-4a37-8ad3-314a9843ea63",
   "metadata": {},
   "source": [
    "## Normalize GoEmotions (train/dev/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78de56b7-bada-4791-b3ce-5b539c1dbc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54263, 6), (54263, 6))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am reusing the loaded splits (go_train, go_dev, go_test) and combining them\n",
    "go_all = pd.concat([go_train, go_dev, go_test], ignore_index=True)\n",
    "\n",
    "# I am giving temporary column names so the code is easier to read\n",
    "go_all = go_all.rename(columns={0: \"user_message\", 1: \"label_id\", 2: \"comment_id\"})\n",
    "\n",
    "# I am cleaning the text in the user_message column\n",
    "go_all[\"user_message\"] = go_all[\"user_message\"].apply(clean_text)\n",
    "\n",
    "# I am creating a temporary atlas_emotion from the numeric label_id\n",
    "# (Later I will map these ids into the ATLAS emotion names)\n",
    "go_all[\"atlas_emotion\"] = go_all[\"label_id\"].astype(str)\n",
    "\n",
    "# I am adding empty need and strategy columns for now\n",
    "go_all[\"need\"] = None\n",
    "go_all[\"strategy\"] = None\n",
    "\n",
    "# I am setting the source name for traceability\n",
    "go_all[\"source\"] = \"goemotions\"\n",
    "\n",
    "# GoEmotions is general conversation, so I am using base safety = 0\n",
    "go_all[\"safety_flag\"] = go_all[\"user_message\"].apply(lambda x: compute_safety_flag(x, base_flag=0))\n",
    "\n",
    "# I am selecting only the columns that belong in the classification bucket\n",
    "go_norm = go_all[[\"user_message\", \"atlas_emotion\", \"need\", \"strategy\", \"safety_flag\", \"source\"]]\n",
    "\n",
    "# I am appending the normalized GoEmotions data to the classification bucket\n",
    "classification_bucket = pd.concat([classification_bucket, go_norm], ignore_index=True)\n",
    "\n",
    "# I am checking the shapes to confirm the number of rows added\n",
    "go_norm.shape, classification_bucket.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f5ef4-e8c8-4229-9508-fc76a8f87079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "345de18d-ecc8-4835-bc96-ce20929148c6",
   "metadata": {},
   "source": [
    "# Normalize DailyDialog\n",
    "\n",
    "## DailyDialog contains dialogue exchanges, but your assistant is message-centered, so you will extract:\n",
    "\n",
    "* each utterance\n",
    "* its emotion label\n",
    "* and treat it as a standalone user message\n",
    "\n",
    "DailyDialog files include:\n",
    "\n",
    "1. train.csv\n",
    "2. validation.csv\n",
    "3. test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375746a-06a6-4db5-9aeb-3819d661215d",
   "metadata": {},
   "source": [
    "## Inspect DailyDialog columns and sample rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce99dd2-ffe8-407e-9868-7f9ca2525254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: ['dialog', 'act', 'emotion']\n",
      "Train shape: (11118, 3)\n",
      "First 3 rows of train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>act</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Say , Jim , how about going for a few beers ...</td>\n",
       "      <td>[3 4 2 2 2 3 4 1 3 4]</td>\n",
       "      <td>[0 0 0 0 0 0 4 4 4 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Can you do push-ups ? '\\n \" Of course I can ...</td>\n",
       "      <td>[2 1 2 2 1 1]</td>\n",
       "      <td>[0 0 6 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Can you study with the radio on ? '\\n ' No ,...</td>\n",
       "      <td>[2 1 2 1 1]</td>\n",
       "      <td>[0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dialog                    act  \\\n",
       "0  ['Say , Jim , how about going for a few beers ...  [3 4 2 2 2 3 4 1 3 4]   \n",
       "1  ['Can you do push-ups ? '\\n \" Of course I can ...          [2 1 2 2 1 1]   \n",
       "2  ['Can you study with the radio on ? '\\n ' No ,...            [2 1 2 1 1]   \n",
       "\n",
       "                 emotion  \n",
       "0  [0 0 0 0 0 0 4 4 4 4]  \n",
       "1          [0 0 6 0 0 0]  \n",
       "2            [0 0 0 0 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation columns: ['dialog', 'act', 'emotion']\n",
      "First 3 rows of validation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>act</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Good morning , sir . Is there a bank near he...</td>\n",
       "      <td>[2 1 3 2 1 2 1]</td>\n",
       "      <td>[0 0 0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Good afternoon . This is Michelle Li speakin...</td>\n",
       "      <td>[2 1 1 1 1 2 3 2 3 4]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['What qualifications should a reporter have ?...</td>\n",
       "      <td>[2 1 2 1]</td>\n",
       "      <td>[0 0 0 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dialog                    act  \\\n",
       "0  ['Good morning , sir . Is there a bank near he...        [2 1 3 2 1 2 1]   \n",
       "1  ['Good afternoon . This is Michelle Li speakin...  [2 1 1 1 1 2 3 2 3 4]   \n",
       "2  ['What qualifications should a reporter have ?...              [2 1 2 1]   \n",
       "\n",
       "                 emotion  \n",
       "0        [0 0 0 0 0 0 0]  \n",
       "1  [0 0 0 0 0 0 0 0 0 0]  \n",
       "2              [0 0 0 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test columns: ['dialog', 'act', 'emotion']\n",
      "First 3 rows of test:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>act</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Hey man , you wanna buy some weed ? ' ' Some...</td>\n",
       "      <td>[3 2 3 4 3 4 3 2 3 4 2 3]</td>\n",
       "      <td>[0 6 0 0 0 0 0 0 0 0 3 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['The taxi drivers are on strike again . ' ' W...</td>\n",
       "      <td>[1 2 1 1]</td>\n",
       "      <td>[0 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"We've managed to reduce our energy consumpti...</td>\n",
       "      <td>[1 2 1 2 1 2 1]</td>\n",
       "      <td>[0 0 0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dialog  \\\n",
       "0  ['Hey man , you wanna buy some weed ? ' ' Some...   \n",
       "1  ['The taxi drivers are on strike again . ' ' W...   \n",
       "2  [\"We've managed to reduce our energy consumpti...   \n",
       "\n",
       "                         act                    emotion  \n",
       "0  [3 2 3 4 3 4 3 2 3 4 2 3]  [0 6 0 0 0 0 0 0 0 0 3 0]  \n",
       "1                  [1 2 1 1]                  [0 0 0 0]  \n",
       "2            [1 2 1 2 1 2 1]            [0 0 0 0 0 0 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I am reloading the DailyDialog splits to inspect their structure\n",
    "dd_train = pd.read_csv(DAILYDIALOG_DIR / \"train.csv\")\n",
    "dd_valid = pd.read_csv(DAILYDIALOG_DIR / \"validation.csv\")\n",
    "dd_test = pd.read_csv(DAILYDIALOG_DIR / \"test.csv\")\n",
    "\n",
    "print(\"Train columns:\", dd_train.columns.tolist())\n",
    "print(\"Train shape:\", dd_train.shape)\n",
    "print(\"First 3 rows of train:\")\n",
    "display(dd_train.head(3))\n",
    "\n",
    "print(\"Validation columns:\", dd_valid.columns.tolist())\n",
    "print(\"First 3 rows of validation:\")\n",
    "display(dd_valid.head(3))\n",
    "\n",
    "print(\"Test columns:\", dd_test.columns.tolist())\n",
    "print(\"First 3 rows of test:\")\n",
    "display(dd_test.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3225e62-fc2d-42b4-984a-fd1b810d466f",
   "metadata": {},
   "source": [
    "## Normalize DailyDialog into classification_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c6e5c-f1fe-4ec4-ac39-e805f64c2336",
   "metadata": {},
   "source": [
    "### This will:\n",
    "\n",
    "1. Parse the dialog string into a real Python list.\n",
    "2. Parse the emotion string into a list of ids.\n",
    "3. Pair each utterance with its emotion.\n",
    "4. Clean the text and compute safety flags.\n",
    "5. Append to classification_bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b7ce49-b7fa-4878-b380-6079148c325d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13118, 6), (67381, 6))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# I am reloading DailyDialog splits with the correct columns\n",
    "dd_train = pd.read_csv(DAILYDIALOG_DIR / \"train.csv\")\n",
    "dd_valid = pd.read_csv(DAILYDIALOG_DIR / \"validation.csv\")\n",
    "dd_test = pd.read_csv(DAILYDIALOG_DIR / \"test.csv\")\n",
    "\n",
    "# I am combining all splits into one DataFrame\n",
    "dd_all = pd.concat([dd_train, dd_valid, dd_test], ignore_index=True)\n",
    "\n",
    "# I am defining helpers to parse the dialog and emotion columns\n",
    "\n",
    "def parse_dialog_list(s):\n",
    "    # I am converting the string representation of a list into a real Python list\n",
    "    try:\n",
    "        items = ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return []\n",
    "    # I am cleaning each utterance and keeping only non-empty ones\n",
    "    return [clean_text(x) for x in items if clean_text(x) != \"\"]\n",
    "\n",
    "def parse_emotion_list(s):\n",
    "    # I am converting strings like \"[0 0 0 4]\" into [\"0\",\"0\",\"0\",\"4\"]\n",
    "    s = str(s).strip()\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        s = s[1:-1]\n",
    "    if not s:\n",
    "        return []\n",
    "    return s.split()\n",
    "\n",
    "# I am applying the parsers\n",
    "dd_all[\"utterances\"] = dd_all[\"dialog\"].apply(parse_dialog_list)\n",
    "dd_all[\"emotion_ids\"] = dd_all[\"emotion\"].apply(parse_emotion_list)\n",
    "\n",
    "# I am expanding each conversation into separate rows\n",
    "expanded_rows = []\n",
    "\n",
    "for _, row in dd_all.iterrows():\n",
    "    utts = row[\"utterances\"]\n",
    "    emos = row[\"emotion_ids\"]\n",
    "    \n",
    "    for i, utt in enumerate(utts):\n",
    "        emo = emos[i] if i < len(emos) else None\n",
    "        \n",
    "        expanded_rows.append({\n",
    "            \"user_message\": utt,\n",
    "            \"atlas_emotion\": emo,   # I will map these ids to ATLAS later\n",
    "            \"need\": None,\n",
    "            \"strategy\": None,\n",
    "            \"safety_flag\": compute_safety_flag(utt, base_flag=0),\n",
    "            \"source\": \"dailydialog\"\n",
    "        })\n",
    "\n",
    "dd_norm = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# I am appending DailyDialog to the classification bucket\n",
    "classification_bucket = pd.concat([classification_bucket, dd_norm], ignore_index=True)\n",
    "\n",
    "# I am checking the shapes to confirm processing\n",
    "dd_norm.shape, classification_bucket.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b4a64-65f4-423c-8299-57584e300d5d",
   "metadata": {},
   "source": [
    "## Normalize EmpatheticDialogues (Facebook AI) into both classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e685d-1d0c-4fd5-9cdb-e1dd86f13bf0",
   "metadata": {},
   "source": [
    "## This dataset is extremely important because it contributes to TWO buckets:\n",
    "\n",
    "1. classification_bucket: Emotion classifier training (user_message → emotion)\n",
    "\n",
    "2. response_bucket: Paired data for the model to learn emotional + empathetic responses\n",
    "(user_message → bot_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf6bc0-7047-4e52-b62d-2826b5451b57",
   "metadata": {},
   "source": [
    "### Inspect EmpatheticDialogues structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21031186-20c0-4bac-b7a8-fcc702cb95ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Unnamed: 0', 'Situation', 'emotion', 'empathetic_dialogues', 'labels', 'Unnamed: 5', 'Unnamed: 6']\n",
      "Shape: (64636, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Situation</th>\n",
       "      <th>emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :I remember going to see the firework...</td>\n",
       "      <td>Was this a friend you were in love with, or ju...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :This was a best friend. I miss her.\\...</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :We no longer talk.\\nAgent :</td>\n",
       "      <td>Oh was this something that happened because of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :Was this a friend you were in love w...</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :Where has she gone?\\nAgent :</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          Situation      emotion  \\\n",
       "0           0  I remember going to the fireworks with my best...  sentimental   \n",
       "1           1  I remember going to the fireworks with my best...  sentimental   \n",
       "2           2  I remember going to the fireworks with my best...  sentimental   \n",
       "3           3  I remember going to the fireworks with my best...  sentimental   \n",
       "4           4  I remember going to the fireworks with my best...  sentimental   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  Customer :I remember going to see the firework...   \n",
       "1  Customer :This was a best friend. I miss her.\\...   \n",
       "2              Customer :We no longer talk.\\nAgent :   \n",
       "3  Customer :Was this a friend you were in love w...   \n",
       "4             Customer :Where has she gone?\\nAgent :   \n",
       "\n",
       "                                              labels Unnamed: 5 Unnamed: 6  \n",
       "0  Was this a friend you were in love with, or ju...        NaN        NaN  \n",
       "1                                Where has she gone?        NaN        NaN  \n",
       "2  Oh was this something that happened because of...        NaN        NaN  \n",
       "3                This was a best friend. I miss her.        NaN        NaN  \n",
       "4                                 We no longer talk.        NaN        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I am loading the EmpatheticDialogues dataset to inspect structure\n",
    "emp = pd.read_csv(EMP_DIALOG_DIR / \"emotion_emotion_69k.csv\")\n",
    "\n",
    "print(\"Columns:\", emp.columns.tolist())\n",
    "print(\"Shape:\", emp.shape)\n",
    "display(emp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef4c94-e78f-4049-83c2-9d4aa67f21f3",
   "metadata": {},
   "source": [
    "## For classification_bucket\n",
    "\n",
    "We want: user_message → atlas_emotion\n",
    "\n",
    "This will come from: \n",
    "* Situation as the user’s initial emotional statement\n",
    "* emotion as the label\n",
    "* Plus also splitting the empathetic_dialogues text into user turns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e920a-3423-44b9-8f7d-64feeebe9396",
   "metadata": {},
   "source": [
    "## For response_bucket\n",
    "\n",
    "I am going to paired data:\n",
    "\n",
    "* user_message = Situation\n",
    "* bot_reply = labels\n",
    "* emotion = emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e3d42-7e3d-448a-a2a9-5796c0d7b45f",
   "metadata": {},
   "source": [
    "## Normalize EmpatheticDialogues into both buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90994dae-405a-4e3b-9429-692bdaafc429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64636, 6), (132017, 6), (64636, 7), (64636, 7))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am loading EmpatheticDialogues dataset\n",
    "emp = pd.read_csv(EMP_DIALOG_DIR / \"emotion_emotion_69k.csv\")\n",
    "\n",
    "# I am cleaning column names and dropping unused columns\n",
    "emp = emp.rename(columns={\n",
    "    \"Situation\": \"situation\",\n",
    "    \"emotion\": \"emotion_label\",\n",
    "    \"empathetic_dialogues\": \"dialog_text\",\n",
    "    \"labels\": \"reply_text\"\n",
    "})\n",
    "\n",
    "emp = emp[[\"situation\", \"emotion_label\", \"dialog_text\", \"reply_text\"]]\n",
    "\n",
    "# I am cleaning all text columns\n",
    "for col in [\"situation\", \"dialog_text\", \"reply_text\"]:\n",
    "    emp[col] = emp[col].apply(clean_text)\n",
    "\n",
    "# 1. ADD TO CLASSIFICATION BUCKET\n",
    "\n",
    "# I am using the \"situation\" as the user_message for emotion classification\n",
    "emp_class = emp.copy()\n",
    "emp_class[\"user_message\"] = emp_class[\"situation\"]\n",
    "emp_class[\"atlas_emotion\"] = emp_class[\"emotion_label\"]\n",
    "emp_class[\"need\"] = None\n",
    "emp_class[\"strategy\"] = None\n",
    "emp_class[\"source\"] = \"empathetic_dialogues\"\n",
    "emp_class[\"safety_flag\"] = emp_class[\"user_message\"].apply(lambda x: compute_safety_flag(x, base_flag=1))\n",
    "\n",
    "emp_class_norm = emp_class[[\"user_message\", \"atlas_emotion\", \"need\", \"strategy\", \"safety_flag\", \"source\"]]\n",
    "\n",
    "classification_bucket = pd.concat([classification_bucket, emp_class_norm], ignore_index=True)\n",
    "\n",
    "# 2. ADD TO RESPONSE BUCKET\n",
    "\n",
    "# I am using: user_message = Situation, bot_reply = labels\n",
    "emp_resp = emp.copy()\n",
    "emp_resp[\"user_message\"] = emp_resp[\"situation\"]\n",
    "emp_resp[\"bot_reply\"] = emp_resp[\"reply_text\"]\n",
    "emp_resp[\"atlas_emotion\"] = emp_resp[\"emotion_label\"]\n",
    "emp_resp[\"need\"] = None\n",
    "emp_resp[\"strategy\"] = None\n",
    "emp_resp[\"source\"] = \"empathetic_dialogues\"\n",
    "emp_resp[\"safety_flag\"] = emp_resp[\"user_message\"].apply(lambda x: compute_safety_flag(x, base_flag=1))\n",
    "\n",
    "emp_resp_norm = emp_resp[[\"user_message\", \"bot_reply\", \"atlas_emotion\", \"need\", \"strategy\", \"safety_flag\", \"source\"]]\n",
    "\n",
    "response_bucket = pd.concat([response_bucket, emp_resp_norm], ignore_index=True)\n",
    "\n",
    "# I am showing shapes to confirm processing\n",
    "emp_class_norm.shape, classification_bucket.shape, emp_resp_norm.shape, response_bucket.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7c486-508f-45f4-a87b-b3faa0fecbcb",
   "metadata": {},
   "source": [
    "## Normalize Mental Health Counseling Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b729cfb-031b-41a8-b443-ec128cf219b8",
   "metadata": {},
   "source": [
    "## This dataset is incredibly important because:\n",
    "\n",
    "1. it improves the emotion classifier\n",
    "2. it enriches the response generator with real counseling patterns\n",
    "3. it helps the safety classifier understand “distress but not suicidal” content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba49b6-be27-45cc-97ef-c2bcb269313a",
   "metadata": {},
   "source": [
    "### Inspect the JSON structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24950615-cc51-4dac-92af-f74828a8562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE 1: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n",
      "LINE 2: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n",
      "LINE 3: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n",
      "LINE 4: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n",
      "LINE 5: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n",
      "LINE 6: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n",
      "LINE 7: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n",
      "LINE 8: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n",
      "LINE 9: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n",
      "LINE 10: {\"Context\":\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplat\n"
     ]
    }
   ],
   "source": [
    "# I am printing the first few lines of the JSON file to understand its format\n",
    "with open(MH_COUNSEL_DIR / \"combined_dataset.json\", \"r\") as f:\n",
    "    for i in range(10):\n",
    "        line = f.readline()\n",
    "        print(f\"LINE {i+1}: {line[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e1add-5679-4a0c-bd69-1bba3f22ecc7",
   "metadata": {},
   "source": [
    "## The dataset contains only two-column, representing:\n",
    "\n",
    "* Context → the seeker’s message\n",
    "* Context_Response → the counselor’s message\n",
    "\n",
    "This fits perfectly for:\n",
    "\n",
    "1. classification_bucket: (user_message = Context)\n",
    "2. response_bucket: (user_message = Context, bot_reply = Context_Response)\n",
    "3. safety_bucket: (Context contains heavy distress → base safety 1, and many lines might escalate to 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad97da-2abc-44ae-a453-3f6b68d2e0c8",
   "metadata": {},
   "source": [
    "### What I will produce from this dataset:\n",
    "\n",
    "1. classification_bucket rows\n",
    "\n",
    "From every seeker message:\n",
    "* user_message = text\n",
    "* atlas_emotion = None (later from emotion model)\n",
    "* need = None\n",
    "* strategy = None\n",
    "* safety_flag = 1  (mental health content)\n",
    "* source = mental_counseling\n",
    "\n",
    "\n",
    "2. response_bucket rows\n",
    "\n",
    "Pair seeker → counselor:\n",
    "* user_message = seeker message\n",
    "* bot_reply = counselor message\n",
    "* atlas_emotion = None\n",
    "* need = None\n",
    "* strategy = None\n",
    "* safety_flag = computed from user message\n",
    "* source = mental_counseling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22fd619-6fc4-433e-88f1-e548a6122ef7",
   "metadata": {},
   "source": [
    "## Load the JSONL mental health dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90813512-4550-4fb4-b6ef-449a863d437a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3512,\n",
       " {'Context': \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\",\n",
       "  'Response': \"If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media. \\xa0Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living. \\xa0They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible. \\xa0 Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\"})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "mh_rows = []\n",
    "\n",
    "# I am reading the JSONL file line by line\n",
    "with open(MH_COUNSEL_DIR / \"combined_dataset.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            mh_rows.append(obj)\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing line:\", line[:200])\n",
    "            continue\n",
    "\n",
    "len(mh_rows), mh_rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410f53e-54e5-4386-99b9-136f7f71062e",
   "metadata": {},
   "source": [
    "### Normalize Mental Health Counseling into all 3 buckets (adding context messages to the safety_bucket to strengthen crisis detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e4b1f85-1e26-4b80-9319-0f64d61ac992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3512, 6), (135529, 6), (3512, 7), (68148, 7), (3512, 3), (3512, 3))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# I am converting the JSONL records into a tidy DataFrame\n",
    "mh_df = pd.DataFrame(mh_rows)\n",
    "\n",
    "# I am cleaning all text entries\n",
    "mh_df[\"Context\"] = mh_df[\"Context\"].apply(clean_text)\n",
    "mh_df[\"Response\"] = mh_df[\"Response\"].apply(clean_text)\n",
    "\n",
    "# 1. ADD TO CLASSIFICATION BUCKET\n",
    "\n",
    "mh_class = pd.DataFrame({\n",
    "    \"user_message\": mh_df[\"Context\"],\n",
    "    \"atlas_emotion\": None,\n",
    "    \"need\": None,\n",
    "    \"strategy\": None,\n",
    "    \"safety_flag\": mh_df[\"Context\"].apply(lambda x: compute_safety_flag(x, base_flag=1)),\n",
    "    \"source\": \"mental_counseling\"\n",
    "})\n",
    "\n",
    "classification_bucket = pd.concat([classification_bucket, mh_class], ignore_index=True)\n",
    "\n",
    "# 2. ADD TO RESPONSE BUCKET\n",
    "\n",
    "mh_resp = pd.DataFrame({\n",
    "    \"user_message\": mh_df[\"Context\"],\n",
    "    \"bot_reply\": mh_df[\"Response\"],\n",
    "    \"atlas_emotion\": None,\n",
    "    \"need\": None,\n",
    "    \"strategy\": None,\n",
    "    \"safety_flag\": mh_df[\"Context\"].apply(lambda x: compute_safety_flag(x, base_flag=1)),\n",
    "    \"source\": \"mental_counseling\"\n",
    "})\n",
    "\n",
    "response_bucket = pd.concat([response_bucket, mh_resp], ignore_index=True)\n",
    "\n",
    "# 3. ADD TO SAFETY BUCKET\n",
    "\n",
    "mh_safety = pd.DataFrame({\n",
    "    \"user_message\": mh_df[\"Context\"],\n",
    "    \"safety_flag\": mh_df[\"Context\"].apply(lambda x: compute_safety_flag(x, base_flag=1)),\n",
    "    \"source\": \"mental_counseling\"\n",
    "})\n",
    "\n",
    "safety_bucket = pd.concat([safety_bucket, mh_safety], ignore_index=True)\n",
    "\n",
    "# Confirm shapes\n",
    "\n",
    "mh_class.shape, classification_bucket.shape, mh_resp.shape, response_bucket.shape, mh_safety.shape, safety_bucket.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfef5cb0-1e09-420b-8535-fa7bec1d612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_bucket columns: ['user_message', 'atlas_emotion', 'need', 'strategy', 'safety_flag', 'source']\n",
      "response_bucket columns: ['user_message', 'bot_reply', 'atlas_emotion', 'need', 'strategy', 'safety_flag', 'source']\n",
      "safety_bucket columns: ['user_message', 'safety_flag', 'source']\n"
     ]
    }
   ],
   "source": [
    "# I am checking the column names of each bucket\n",
    "print(\"classification_bucket columns:\", classification_bucket.columns.tolist())\n",
    "print(\"response_bucket columns:\", response_bucket.columns.tolist())\n",
    "print(\"safety_bucket columns:\", safety_bucket.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443414e-f427-43a2-acd5-7324cdca1ab4",
   "metadata": {},
   "source": [
    "## Suicide and Depression Detection dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae63732-8ef7-46e1-a7ff-7745e99a0185",
   "metadata": {},
   "source": [
    "### Inspect Suicide and Depression Detection dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dff8fd-3518-450b-809c-eec1172e2391",
   "metadata": {},
   "source": [
    "How I will use this dataset:\n",
    "\n",
    "1. classification_bucket\n",
    "\n",
    "Keep all messages: user_message - atlas_emotion - need\tstrategy - safety_flag - source\n",
    "\n",
    "* atlas_emotion = None\n",
    "* need and strategy = None\n",
    "* safety_flag =\n",
    "\n",
    "     * 2: if labeled suicidal\n",
    "     * 1: otherwise\n",
    "\n",
    "2. safety_bucket\n",
    "\n",
    "Only the message itself: user_message - safety_flag - source \n",
    "\n",
    "3. response_bucket: This dataset does NOT provide bot replies, so I will not add anything to response_bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "190c174a-02ac-43dc-b02a-322ca21761f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Unnamed: 0', 'text', 'class']\n",
      "Shape: (232074, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>I’m so lostHello, my name is Adam (16) and I’v...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text        class\n",
       "0           2  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "1           3  Am I weird I don't get affected by compliments...  non-suicide\n",
       "2           4  Finally 2020 is almost over... So I can never ...  non-suicide\n",
       "3           8          i need helpjust help me im crying so hard      suicide\n",
       "4           9  I’m so lostHello, my name is Adam (16) and I’v...      suicide"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I am loading the suicide detection dataset to inspect structure\n",
    "sui = pd.read_csv(SUICIDE_DIR / \"Suicide_Detection.csv\")\n",
    "\n",
    "print(\"Columns:\", sui.columns.tolist())\n",
    "print(\"Shape:\", sui.shape)\n",
    "display(sui.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6552a145-3204-40ff-a008-a63e54fba95a",
   "metadata": {},
   "source": [
    "### Suicide and Depression Detection dataset contains:\n",
    "\n",
    "Columns:\n",
    "* \"text\" → the user message\n",
    "* \"class\" → either \"suicide\" or \"non-suicide\"\n",
    "* \"Unnamed: 0\" → ignore\n",
    "\n",
    "Shape:\n",
    "232,074 rows — large and very useful for training safety detection\n",
    "\n",
    "This is excellent for strengthening your safety classifier and for balancing distress signals in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bdca4b-14be-49d8-9d95-1cb96e716250",
   "metadata": {},
   "source": [
    "### Normalize Suicide and Depression Detection dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac31755-d23b-45c3-8fb2-40a7816ab5e6",
   "metadata": {},
   "source": [
    "This dataset goes into:\n",
    "\n",
    "1. classification_bucket: For general emotional classification and safety training.\n",
    "2. safety_bucket: For crisis detection.\n",
    "3. NOT included in response_bucket: (no bot reply available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0de3ee4e-0614-4c34-bc91-6bea327e16b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((232074, 6), (367603, 6), (232074, 3), (235586, 3))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am cleaning the suicide detection dataset\n",
    "\n",
    "# Keeping only the relevant columns\n",
    "sui = sui.rename(columns={\"text\": \"user_message\", \"class\": \"label\"})\n",
    "sui[\"user_message\"] = sui[\"user_message\"].apply(clean_text)\n",
    "\n",
    "# I am computing a safety flag based on the dataset label\n",
    "def suicide_label_to_flag(label):\n",
    "    if label == \"suicide\":\n",
    "        return 2\n",
    "    return 1   # non-suicide still indicates mental health content\n",
    "\n",
    "sui[\"safety_flag\"] = sui[\"label\"].apply(suicide_label_to_flag)\n",
    "\n",
    "# 1. ADD TO CLASSIFICATION BUCKET\n",
    "\n",
    "sui_class = pd.DataFrame({\n",
    "    \"user_message\": sui[\"user_message\"],\n",
    "    \"atlas_emotion\": None,\n",
    "    \"need\": None,\n",
    "    \"strategy\": None,\n",
    "    \"safety_flag\": sui[\"safety_flag\"],\n",
    "    \"source\": \"suicide_detection\"\n",
    "})\n",
    "\n",
    "classification_bucket = pd.concat([classification_bucket, sui_class], ignore_index=True)\n",
    "\n",
    "# 2. ADD TO SAFETY BUCKET\n",
    "\n",
    "sui_safety = pd.DataFrame({\n",
    "    \"user_message\": sui[\"user_message\"],\n",
    "    \"safety_flag\": sui[\"safety_flag\"],\n",
    "    \"source\": \"suicide_detection\"\n",
    "})\n",
    "\n",
    "safety_bucket = pd.concat([safety_bucket, sui_safety], ignore_index=True)\n",
    "\n",
    "# Confirm shapes\n",
    "\n",
    "sui_class.shape, classification_bucket.shape, sui_safety.shape, safety_bucket.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac557d1-7f2e-4fdc-ab6b-23e710cb6b65",
   "metadata": {},
   "source": [
    "## MentalChat_16K (Synthetic_Data_10K + Interview_Data_6K) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31b75256-56b7-4da1-aa3e-fe4c3649691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Data Columns: ['instruction', 'input', 'output']\n",
      "Interview Data Columns: ['instruction', 'input', 'output']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>I think I might be developing a substance abus...</td>\n",
       "      <td>I'm really glad that you reached out and share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>Parenting has become such a challenge for me. ...</td>\n",
       "      <td>I can understand how challenging parenting can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>Intimacy has always been a struggle for me. I ...</td>\n",
       "      <td>I can understand how challenging it must be fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>I've been struggling with substance abuse for ...</td>\n",
       "      <td>I'm really glad that you reached out and share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>Being a parent is overwhelming and exhausting....</td>\n",
       "      <td>Parenting can definitely be overwhelming and e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  You are a helpful mental health counselling as...   \n",
       "1  You are a helpful mental health counselling as...   \n",
       "2  You are a helpful mental health counselling as...   \n",
       "3  You are a helpful mental health counselling as...   \n",
       "4  You are a helpful mental health counselling as...   \n",
       "\n",
       "                                               input  \\\n",
       "0  I think I might be developing a substance abus...   \n",
       "1  Parenting has become such a challenge for me. ...   \n",
       "2  Intimacy has always been a struggle for me. I ...   \n",
       "3  I've been struggling with substance abuse for ...   \n",
       "4  Being a parent is overwhelming and exhausting....   \n",
       "\n",
       "                                              output  \n",
       "0  I'm really glad that you reached out and share...  \n",
       "1  I can understand how challenging parenting can...  \n",
       "2  I can understand how challenging it must be fo...  \n",
       "3  I'm really glad that you reached out and share...  \n",
       "4  Parenting can definitely be overwhelming and e...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>I've been struggling with my mental health for...</td>\n",
       "      <td>I understand that you've been dealing with a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>I've been feeling overwhelmed with my caregivi...</td>\n",
       "      <td>Your situation is complex, and it's important ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>I've been feeling constantly anxious and unabl...</td>\n",
       "      <td>I can see that you're dealing with a great dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>My mom has Alzheimer's, and I've been her prim...</td>\n",
       "      <td>I'm sorry to hear that your siblings' demands ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a helpful mental health counselling as...</td>\n",
       "      <td>I've tried setting boundaries, but it feels li...</td>\n",
       "      <td>Your concerns are valid, and it's crucial to p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  You are a helpful mental health counselling as...   \n",
       "1  You are a helpful mental health counselling as...   \n",
       "2  You are a helpful mental health counselling as...   \n",
       "3  You are a helpful mental health counselling as...   \n",
       "4  You are a helpful mental health counselling as...   \n",
       "\n",
       "                                               input  \\\n",
       "0  I've been struggling with my mental health for...   \n",
       "1  I've been feeling overwhelmed with my caregivi...   \n",
       "2  I've been feeling constantly anxious and unabl...   \n",
       "3  My mom has Alzheimer's, and I've been her prim...   \n",
       "4  I've tried setting boundaries, but it feels li...   \n",
       "\n",
       "                                              output  \n",
       "0  I understand that you've been dealing with a s...  \n",
       "1  Your situation is complex, and it's important ...  \n",
       "2  I can see that you're dealing with a great dea...  \n",
       "3  I'm sorry to hear that your siblings' demands ...  \n",
       "4  Your concerns are valid, and it's crucial to p...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "syn = pd.read_csv(MENTALCHAT_DIR / \"Synthetic_Data_10K.csv\")\n",
    "intv = pd.read_csv(MENTALCHAT_DIR / \"Interview_Data_6K.csv\")\n",
    "\n",
    "print(\"Synthetic Data Columns:\", syn.columns.tolist())\n",
    "print(\"Interview Data Columns:\", intv.columns.tolist())\n",
    "\n",
    "display(syn.head())\n",
    "display(intv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c69e147-b62c-456a-8e4d-20920d4d366b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['instruction', 'input', 'output'], dtype='object')\n",
      "Index(['instruction', 'input', 'output'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(syn.columns)\n",
    "print(intv.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e75d0a-3cd2-4ffc-8ba0-0b717c9c2ab2",
   "metadata": {},
   "source": [
    "## How I will use this dataset:\n",
    "1. classification_bucket: Use each input message as emotional training data.\n",
    "\n",
    "Keep all messages with the following fields:\n",
    "* user_message = input\n",
    "* atlas_emotion = None\n",
    "* need = None\n",
    "* strategy = None\n",
    "* safety_flag = computed from the user message\n",
    "* source = mentalchat_16k\n",
    "\n",
    "2. response_bucket: This dataset provides supervised pairs of (input → output).\n",
    "For each record:\n",
    "* user_message = input\n",
    "* bot_reply = output\n",
    "* atlas_emotion = None\n",
    "* need = None\n",
    "* strategy = None\n",
    "* safety_flag = computed from the user message\n",
    "* source = mentalchat_16k\n",
    "\n",
    "3. safety_bucket: Because many MentalChat inputs contain distress signals, I will also store each input inside the safety bucket.\n",
    "* user_message = input\n",
    "* safety_flag = computed from the user message\n",
    "* source = mentalchat_16k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e7656-ae5b-40f0-b195-0bd8c482bced",
   "metadata": {},
   "source": [
    "## Normalize MentalChat 16K (Synthetic + Interview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "088ce48e-45fc-43b5-bf5c-9491e21221ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((383687, 6), (84232, 7), (251670, 3))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize MentalChat 16K (Synthetic + Interview)\n",
    "\n",
    "def normalize_mentalchat(df, source_name):\n",
    "    # Clean text\n",
    "    df[\"instruction\"] = df[\"instruction\"].apply(clean_text)\n",
    "    df[\"input\"] = df[\"input\"].apply(clean_text)\n",
    "    df[\"output\"] = df[\"output\"].apply(clean_text)\n",
    "    \n",
    "    # Compute safety flag based on the input message\n",
    "    df[\"safety_flag\"] = df[\"input\"].apply(lambda x: compute_safety_flag(x, base_flag=0))\n",
    "    \n",
    "    # 1. Add to CLASSIFICATION BUCKET \n",
    "    mc_class = pd.DataFrame({\n",
    "        \"user_message\": df[\"input\"],\n",
    "        \"atlas_emotion\": None,\n",
    "        \"need\": None,\n",
    "        \"strategy\": None,\n",
    "        \"safety_flag\": df[\"safety_flag\"],\n",
    "        \"source\": source_name\n",
    "    })\n",
    "    \n",
    "    # 2. Add to RESPONSE BUCKET \n",
    "    mc_resp = pd.DataFrame({\n",
    "        \"user_message\": df[\"input\"],\n",
    "        \"bot_reply\": df[\"output\"],\n",
    "        \"atlas_emotion\": None,\n",
    "        \"need\": None,\n",
    "        \"strategy\": None,\n",
    "        \"safety_flag\": df[\"safety_flag\"],\n",
    "        \"source\": source_name\n",
    "    })\n",
    "    \n",
    "    # 3. Add to SAFETY BUCKET \n",
    "    mc_safety = pd.DataFrame({\n",
    "        \"user_message\": df[\"input\"],\n",
    "        \"safety_flag\": df[\"safety_flag\"],\n",
    "        \"source\": source_name\n",
    "    })\n",
    "    \n",
    "    return mc_class, mc_resp, mc_safety\n",
    "\n",
    "\n",
    "# Process Synthetic 10k\n",
    "mc_syn_class, mc_syn_resp, mc_syn_safety = normalize_mentalchat(syn, \"mentalchat_synthetic\")\n",
    "\n",
    "classification_bucket = pd.concat([classification_bucket, mc_syn_class], ignore_index=True)\n",
    "response_bucket = pd.concat([response_bucket, mc_syn_resp], ignore_index=True)\n",
    "safety_bucket = pd.concat([safety_bucket, mc_syn_safety], ignore_index=True)\n",
    "\n",
    "# Process Interview 6k\n",
    "mc_int_class, mc_int_resp, mc_int_safety = normalize_mentalchat(intv, \"mentalchat_interview\")\n",
    "\n",
    "classification_bucket = pd.concat([classification_bucket, mc_int_class], ignore_index=True)\n",
    "response_bucket = pd.concat([response_bucket, mc_int_resp], ignore_index=True)\n",
    "safety_bucket = pd.concat([safety_bucket, mc_int_safety], ignore_index=True)\n",
    "\n",
    "# Show final shapes\n",
    "classification_bucket.shape, response_bucket.shape, safety_bucket.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a49c90-c988-4e28-a9f9-f69ec66935e3",
   "metadata": {},
   "source": [
    "## Summary of normalization for all 6 external datasets\n",
    "\n",
    "This section documents how each raw dataset is mapped into the three master buckets:\n",
    "- classification_bucket  (user_message + labels/features)\n",
    "- response_bucket        (user_message, bot_reply + labels/features)\n",
    "- safety_bucket          (messages used for safety / crisis detection)\n",
    "\n",
    "---\n",
    "\n",
    "1. GoEmotions (Reddit comments)\n",
    "\n",
    "* Source files: train.tsv, dev.tsv, test.tsv\n",
    "* I combined all splits into a single DataFrame.\n",
    "* For each row I used:\n",
    "  - user_message = cleaned comment text\n",
    "  - atlas_emotion = string version of the numeric label_id (temporary, to be mapped to ATLAS later)\n",
    "  - need = None\n",
    "  - strategy = None\n",
    "  - safety_flag = computed from user_message with base_flag = 0\n",
    "  - source = goemotions\n",
    "* Buckets:\n",
    "  - Added rows only to classification_bucket.\n",
    "  - Did not add explicit rows to response_bucket or safety_bucket for this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "2. DailyDialog (everyday conversations)\n",
    "\n",
    "* Source files: train.csv, validation.csv, test.csv\n",
    "* Each row contains:\n",
    "  - dialog  = list of utterances as a string\n",
    "  - act     = dialog act sequence (not used yet)\n",
    "  - emotion = emotion id sequence (not used yet)\n",
    "* Normalization:\n",
    "  - I parsed the dialog field into a list of utterances.\n",
    "  - I expanded each conversation into one row per utterance.\n",
    "  - For now I kept:\n",
    "    - user_message = single utterance text\n",
    "    - atlas_emotion = None (emotion ids kept for later mapping)\n",
    "    - need = None\n",
    "    - strategy = None\n",
    "    - safety_flag = computed from user_message with base_flag = 0\n",
    "    - source = dailydialog\n",
    "* Buckets:\n",
    "  - Added rows only to classification_bucket.\n",
    "\n",
    "---\n",
    "\n",
    "3. EmpatheticDialogues (Facebook AI)\n",
    "\n",
    "* Source file: emotion_emotion_69k.csv\n",
    "* I renamed columns:\n",
    "  - \"Situation\" -> situation\n",
    "  - \"emotion\"  -> emotion_label\n",
    "  - \"empathetic_dialogues\" -> dialog_text\n",
    "  - \"labels\"   -> reply_text\n",
    "* Normalization for classification:\n",
    "  - user_message = situation\n",
    "  - atlas_emotion = emotion_label (as string)\n",
    "  - need = None\n",
    "  - strategy = None\n",
    "  - safety_flag = computed from user_message with base_flag = 1\n",
    "  - source = empathetic_dialogues\n",
    "* Normalization for response:\n",
    "  - user_message = situation\n",
    "  - bot_reply = reply_text\n",
    "  - atlas_emotion = emotion_label\n",
    "  - need = None\n",
    "  - strategy = None\n",
    "  - safety_flag = computed from user_message with base_flag = 1\n",
    "  - source = empathetic_dialogues\n",
    "* Buckets:\n",
    "  - Added rows to classification_bucket and response_bucket.\n",
    "  - No separate rows in safety_bucket (safety_flag is still stored as a feature).\n",
    "\n",
    "---\n",
    "\n",
    "4. Mental Health Counseling Conversations (JSONL)\n",
    "\n",
    "* Source file: combined_dataset.json (JSONL format)\n",
    "* Each line contains:\n",
    "  - \"Context\"  = seeker’s message\n",
    "  - \"Response\" = counselor’s message\n",
    "* After loading line by line, I created a DataFrame with:\n",
    "  - Context, Response\n",
    "  - Both fields cleaned with clean_text.\n",
    "* classification_bucket rows:\n",
    "  - user_message = Context\n",
    "  - atlas_emotion = None\n",
    "  - need = None\n",
    "  - strategy = None\n",
    "  - safety_flag = computed from Context with base_flag = 1\n",
    "  - source = mental_counseling\n",
    "* response_bucket rows:\n",
    "  - user_message = Context\n",
    "  - bot_reply = Response\n",
    "  - atlas_emotion = None\n",
    "  - need = None\n",
    "  - strategy = None\n",
    "  - safety_flag = computed from Context with base_flag = 1\n",
    "  - source = mental_counseling\n",
    "* safety_bucket rows:\n",
    "  - user_message = Context\n",
    "  - safety_flag = computed from Context with base_flag = 1\n",
    "  - source = mental_counseling\n",
    "\n",
    "---\n",
    "\n",
    "5. Suicide and Depression Detection dataset\n",
    "\n",
    "* Source file: Suicide_Detection.csv\n",
    "* Columns used: \"text\", \"class\"\n",
    "* I renamed:\n",
    "  - text  -> user_message\n",
    "  - class -> label\n",
    "* Safety mapping:\n",
    "  - safety_flag = 2 if label == \"suicide\"\n",
    "  - safety_flag = 1 otherwise (non-suicidal mental-health content)\n",
    "* classification_bucket rows:\n",
    "  - user_message = cleaned text\n",
    "  - atlas_emotion = None\n",
    "  - need = None\n",
    "  - strategy = None\n",
    "  - safety_flag = mapped from label\n",
    "  - source = suicide_detection\n",
    "* safety_bucket rows:\n",
    "  - user_message = cleaned text\n",
    "  - safety_flag = mapped from label\n",
    "  - source = suicide_detection\n",
    "* No rows added to response_bucket (there are no bot replies).\n",
    "\n",
    "---\n",
    "\n",
    "6. MentalChat 16K (Synthetic + Interview)\n",
    "\n",
    "* Source files:\n",
    "  - Synthetic_Data_10K.csv\n",
    "  - Interview_Data_6K.csv\n",
    "* Columns: instruction, input, output\n",
    "  - instruction = system / task instruction\n",
    "  - input       = user message\n",
    "  - output      = counselor / assistant reply\n",
    "* For both files I:\n",
    "  - cleaned instruction, input, and output with clean_text\n",
    "  - computed safety_flag from input with base_flag = 0\n",
    "* classification_bucket rows:\n",
    "  - user_message = input\n",
    "  - atlas_emotion = None\n",
    "  - need = None\n",
    "  - strategy = None\n",
    "  - safety_flag = computed from input\n",
    "  - source = mentalchat_synthetic or mentalchat_interview\n",
    "* response_bucket rows:\n",
    "  - user_message = input\n",
    "  - bot_reply = output\n",
    "  - atlas_emotion = None\n",
    "  - need = None\n",
    "  - strategy = None\n",
    "  - safety_flag = computed from input\n",
    "  - source = mentalchat_synthetic or mentalchat_interview\n",
    "* safety_bucket rows:\n",
    "  - user_message = input\n",
    "  - safety_flag = computed from input\n",
    "  - source = mentalchat_synthetic or mentalchat_interview\n",
    "\n",
    "---\n",
    "\n",
    "## Current bucket sizes after normalizing all 6 datasets\n",
    "\n",
    "* classification_bucket.shape = (838687, 6)\n",
    "* response_bucket.shape       = (843232, 7)\n",
    "* safety_bucket.shape         = (251670, 3)\n",
    "\n",
    "These shapes confirm that all datasets have been successfully merged into the three master buckets using a consistent schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb5c47-c354-4572-9063-d3b10a09dffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d010e458-52b8-4d92-9a7d-e76dfbbee402",
   "metadata": {},
   "source": [
    "### Create the export folder (normalized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfaa1710-96aa-4290-9ee9-b8cf09aa1e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export directory created at: /Users/jorgemartinez/Desktop/7_FullStack/Final_Project/1_Datasets/normalized_dataset\n"
     ]
    }
   ],
   "source": [
    "# I am creating the export directory inside 1_Datasets\n",
    "EXPORT_DIR = BASE_DIR / \"normalized_dataset\"\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Export directory created at:\", EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd8993-126f-49b0-a923-8de82e1dca74",
   "metadata": {},
   "source": [
    "### Export the 3 normalized bucket files as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a16b60eb-af9b-41da-87e0-a1fb93508fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files exported:\n",
      "- /Users/jorgemartinez/Desktop/7_FullStack/Final_Project/1_Datasets/normalized_dataset/classification_bucket.csv\n",
      "- /Users/jorgemartinez/Desktop/7_FullStack/Final_Project/1_Datasets/normalized_dataset/response_bucket.csv\n",
      "- /Users/jorgemartinez/Desktop/7_FullStack/Final_Project/1_Datasets/normalized_dataset/safety_bucket.csv\n"
     ]
    }
   ],
   "source": [
    "# I am exporting the three buckets as CSV files\n",
    "classification_path = EXPORT_DIR / \"classification_bucket.csv\"\n",
    "response_path = EXPORT_DIR / \"response_bucket.csv\"\n",
    "safety_path = EXPORT_DIR / \"safety_bucket.csv\"\n",
    "\n",
    "classification_bucket.to_csv(classification_path, index=False)\n",
    "response_bucket.to_csv(response_path, index=False)\n",
    "safety_bucket.to_csv(safety_path, index=False)\n",
    "\n",
    "print(\"Files exported:\")\n",
    "print(\"-\", classification_path)\n",
    "print(\"-\", response_path)\n",
    "print(\"-\", safety_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ce597f-40a1-46ee-ab59-123e0ac8c1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5df2bf-b844-44b4-9dc9-7c93084aff71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978be3b-186e-452c-a387-224ed28c8576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c60be-2334-45c5-ae32-0b3d4422a9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
